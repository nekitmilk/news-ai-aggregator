import json
import redis
import pika
import logging
import threading
from queue import Queue
from manager import SummarizerManager
from categorizer_manager import CategorizerManager  # üëà –¥–æ–±–∞–≤–ª—è–µ–º –∏–º–ø–æ—Ä—Ç

logger = logging.getLogger("NewsConsumer")
logger.setLevel(logging.INFO)
ch = logging.StreamHandler()
formatter = logging.Formatter("[%(asctime)s] [%(levelname)s] %(message)s")
ch.setFormatter(formatter)
logger.addHandler(ch)


class NewsConsumer:
    def __init__(self, conf: dict):
        self.conf = conf

        self.redis_client = redis.StrictRedis(
            host=conf['redis']['host'],
            port=conf['redis']['port'],
            db=conf['redis'].get('db', 0),
            decode_responses=True
        )
        logger.info("‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Redis —É—Å–ø–µ—à–Ω–æ")

        self.rabbit_connection = pika.BlockingConnection(
            pika.ConnectionParameters(
                host=conf['rabbitmq']['host'],
                port=conf['rabbitmq'].get('port', 5672),
                credentials=pika.PlainCredentials(
                    conf['rabbitmq']['user'],
                    conf['rabbitmq']['password']
                ),
                heartbeat=600,
                blocked_connection_timeout=300
            )
        )
        self.channel = self.rabbit_connection.channel()
        self.channel.queue_declare(queue=conf['rabbitmq']['queue'], durable=True)
        self.channel.basic_qos(prefetch_count=1)
        logger.info("‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ RabbitMQ —É—Å–ø–µ—à–Ω–æ")

        # --- Summarizer ---
        logger.info("‚è≥ –ó–∞–≥—Ä—É–∂–∞–µ–º Summarizer –Ω–∞ GPU...")
        self.summarizer_manager = SummarizerManager(device=0)
        logger.info("‚úÖ Summarizer –∑–∞–≥—Ä—É–∂–µ–Ω")

        # --- Categorizer ---
        logger.info("‚è≥ –ó–∞–≥—Ä—É–∂–∞–µ–º Categorizer –Ω–∞ GPU...")
        self.categorizer_manager = CategorizerManager(device=0)
        logger.info("‚úÖ Categorizer –∑–∞–≥—Ä—É–∂–µ–Ω")

        self.model_queue = Queue()
        self.model_thread = threading.Thread(target=self.model_worker, daemon=True)
        self.model_thread.start()

        self.channel.basic_consume(
            queue=conf['rabbitmq']['queue'],
            on_message_callback=self.callback,
            auto_ack=False
        )

    def model_worker(self):
        """–û—Ç–¥–µ–ª—å–Ω—ã–π –ø–æ—Ç–æ–∫ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –º–æ–¥–µ–ª—å—é"""
        while True:
            key, text, ch, method = self.model_queue.get()
            try:
                # --- Summarization ---
                summary, duration = self.summarizer_manager.summarize(text)
                logger.info(f"[SUMMARY]: {summary}")
                logger.info(f"‚è± –í—Ä–µ–º—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏: {duration:.2f} —Å–µ–∫")

                # --- Categorization ---
                categories, cat_duration = self.categorizer_manager.categorize(summary)
                best_cat = categories[0]["label"] if categories else ""
                score = categories[0]["score"] if categories else 0
                logger.info(f"[CATEGORY]: {best_cat} (score={score:.2f})")
                logger.info(f"‚è± –í—Ä–µ–º—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏–∏: {cat_duration:.2f} —Å–µ–∫")

                # --- –§–æ—Ä–º–∏—Ä—É–µ–º JSON –¥–ª—è –ø—Ä–æ–¥—é—Å–µ—Ä–∞ ---
                processed_news = {
                    "id": str(uuid.uuid4()),
                    "title": self.redis_client.get(key) and json.loads(self.redis_client.get(key)).get('header', ''),
                    "summary": summary,
                    "category": best_cat,
                    "source": self.redis_client.get(key) and json.loads(self.redis_client.get(key)).get('source_name', ''),
                    "url": self.redis_client.get(key) and json.loads(self.redis_client.get(key)).get('url', ''),
                    "date": self.redis_client.get(key) and json.loads(self.redis_client.get(key)).get('date', '')
                }

                # --- –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤ –æ—Ç–¥–µ–ª—å–Ω—É—é –æ—á–µ—Ä–µ–¥—å ---
                self.send_to_processed_queue(processed_news)

                ch.basic_ack(delivery_tag=method.delivery_tag)

            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ç–µ–∫—Å—Ç–∞: {e}", exc_info=True)
                ch.basic_nack(delivery_tag=method.delivery_tag, requeue=False)

            finally:
                self.model_queue.task_done()

    def send_to_processed_queue(self, news_json: dict):
        """–û—Ç–ø—Ä–∞–≤–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–π –Ω–æ–≤–æ—Å—Ç–∏ –≤ –æ—Ç–¥–µ–ª—å–Ω—É—é –æ—á–µ—Ä–µ–¥—å"""
        queue_name = self.conf['rabbitmq'].get('processed_queue', 'processed_news')
        self.channel.queue_declare(queue=queue_name, durable=True)
        self.channel.basic_publish(
            exchange='',
            routing_key=queue_name,
            body=json.dumps(news_json),
            properties=pika.BasicProperties(
                delivery_mode=2  # —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –ø—Ä–∏ –ø–∞–¥–µ–Ω–∏–∏ —Å–µ—Ä–≤–µ—Ä–∞
            )
        )
        logger.info(f"üì§ –ù–æ–≤–æ—Å—Ç—å –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–∞ –≤ –æ—á–µ—Ä–µ–¥—å '{queue_name}'")

    def start_consuming(self):
        logger.info("üöÄ –ö–æ–Ω—Å—é–º–µ—Ä –∑–∞–ø—É—â–µ–Ω. –û–∂–∏–¥–∞–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏–π...")
        try:
            self.channel.start_consuming()
        except KeyboardInterrupt:
            logger.info("üõë –ö–æ–Ω—Å—é–º–µ—Ä –∑–∞–≤–µ—Ä—à–∞–µ—Ç —Ä–∞–±–æ—Ç—É.")
        except pika.exceptions.StreamLostError as e:
            logger.warning(f"‚ö†Ô∏è –°–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å RabbitMQ –ø–æ—Ç–µ—Ä—è–Ω–æ: {e}, –ø–µ—Ä–µ–ø–æ–¥–∫–ª—é—á–∞–µ–º—Å—è...")
            self.__init__(self.conf)
            self.start_consuming()

    def callback(self, ch, method, properties, body):
        key = body.decode('utf-8')
        logger.info(f"üì© –ü–æ–ª—É—á–µ–Ω –∫–ª—é—á –∏–∑ RabbitMQ: {key}")

        news_json = self.redis_client.get(key)
        if not news_json:
            logger.warning(f"‚ö†Ô∏è –ö–ª—é—á '{key}' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ Redis")
            ch.basic_ack(delivery_tag=method.delivery_tag)
            return

        news_data = json.loads(news_json)
        header = news_data.get('header')
        text = news_data.get('text', "")

        logger.info(f"üì∞ –ó–∞–≥–æ–ª–æ–≤–æ–∫: {header}")
        logger.info(f"üìÖ –î–∞—Ç–∞: {news_data.get('date')}")
        logger.info(f"üîó URL: {news_data.get('url')}")

        if text.strip():
            try:
                summary, duration = self.summarizer_manager.summarize(text)
                logger.info(f"[SUMMARY]: {summary}")
                logger.info(f"‚è± –í—Ä–µ–º—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏: {duration:.2f} —Å–µ–∫")

                categories, cat_duration = self.categorizer_manager.categorize(summary)
                if categories:
                    best_cat = categories[0]["label"]
                    score = categories[0]["score"]
                    logger.info(f"[CATEGORY]: {best_cat} (score={score:.2f})")
                logger.info(f"‚è± –í—Ä–µ–º—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏–∏: {cat_duration:.2f} —Å–µ–∫")

            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ç–µ–∫—Å—Ç–∞: {e}", exc_info=True)
            finally:
                ch.basic_ack(delivery_tag=method.delivery_tag)
        else:
            logger.warning("‚ö†Ô∏è –¢–µ–∫—Å—Ç –Ω–æ–≤–æ—Å—Ç–∏ –ø—É—Å—Ç–æ–π, —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è –ø—Ä–æ–ø—É—â–µ–Ω–∞")
            ch.basic_ack(delivery_tag=method.delivery_tag)


if __name__ == "__main__":
    with open("config.json", 'r', encoding='utf-8') as file:
        conf = json.load(file)

    consumer = NewsConsumer(conf)
    consumer.start_consuming()
