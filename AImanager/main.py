import json
import redis
import pika
import logging
import threading
import time
from queue import Queue
from manager import SummarizerManager
from categorizer_manager import CategorizerManager
import uuid

logger = logging.getLogger("NewsConsumer")
logger.setLevel(logging.INFO)
ch = logging.StreamHandler()
formatter = logging.Formatter("[%(asctime)s] [%(levelname)s] %(message)s")
ch.setFormatter(formatter)
logger.addHandler(ch)


class NewsConsumer:
    def __init__(self, conf: dict):
        self.conf = conf

        # --- Redis ---
        self.redis_client = redis.StrictRedis(
            host=conf['redis']['host'],
            port=conf['redis']['port'],
            db=conf['redis'].get('db', 0),
            decode_responses=True
        )
        logger.info("‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Redis —É—Å–ø–µ—à–Ω–æ")

        # --- Summarizer –∏ Categorizer ---
        self.summarizer_manager = SummarizerManager(device=0)
        self.categorizer_manager = CategorizerManager(device=0)

        # --- RabbitMQ ---
        self.connect_rabbitmq()

    def connect_rabbitmq(self):
        """–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ RabbitMQ —Å retry"""
        while True:
            try:
                self.rabbit_connection = pika.BlockingConnection(
                    pika.ConnectionParameters(
                        host=self.conf['rabbitmq']['host'],
                        port=self.conf['rabbitmq'].get('port', 5672),
                        credentials=pika.PlainCredentials(
                            self.conf['rabbitmq']['user'],
                            self.conf['rabbitmq']['password']
                        ),
                        heartbeat=600,
                        blocked_connection_timeout=300
                    )
                )
                self.channel = self.rabbit_connection.channel()

                # –û—Å–Ω–æ–≤–Ω—ã–µ –æ—á–µ—Ä–µ–¥–∏
                self.queue_name = self.conf['rabbitmq']['queue']
                self.processed_queue_name = self.conf['rabbitmq'].get('processed_queue', 'processed_news')
                self.channel.queue_declare(queue=self.queue_name, durable=True)
                self.channel.queue_declare(queue=self.processed_queue_name, durable=True)
                self.channel.basic_qos(prefetch_count=1)

                # –ü–æ–¥–ø–∏—Å–∫–∞ –Ω–∞ –æ—á–µ—Ä–µ–¥—å
                self.channel.basic_consume(
                    queue=self.queue_name,
                    on_message_callback=self.callback,
                    auto_ack=False
                )

                logger.info("‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ RabbitMQ —É—Å–ø–µ—à–Ω–æ")
                break
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ RabbitMQ: {e}, –ø–æ–≤—Ç–æ—Ä —á–µ—Ä–µ–∑ 5 —Å–µ–∫")
                time.sleep(5)

    def start_consuming(self):
        logger.info("üöÄ –ö–æ–Ω—Å—é–º–µ—Ä –∑–∞–ø—É—â–µ–Ω. –û–∂–∏–¥–∞–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏–π...")
        while True:
            try:
                self.channel.start_consuming()
            except KeyboardInterrupt:
                logger.info("üõë –ö–æ–Ω—Å—é–º–µ—Ä –∑–∞–≤–µ—Ä—à–∞–µ—Ç —Ä–∞–±–æ—Ç—É.")
                break
            except (pika.exceptions.StreamLostError, pika.exceptions.ConnectionClosed, pika.exceptions.AMQPConnectionError) as e:
                logger.warning(f"‚ö†Ô∏è –°–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å RabbitMQ –ø–æ—Ç–µ—Ä—è–Ω–æ: {e}, –ø–µ—Ä–µ–ø–æ–¥–∫–ª—é—á–∞–µ–º—Å—è...")
                self.connect_rabbitmq()

    def callback(self, ch, method, properties, body):
        key = body.decode('utf-8')
        news_json_raw = self.redis_client.get(key)
        if not news_json_raw:
            logger.warning(f"‚ö†Ô∏è –ö–ª—é—á '{key}' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ Redis")
            ch.basic_ack(delivery_tag=method.delivery_tag)
            return

        news_data = json.loads(news_json_raw)
        text = news_data.get('text', "")

        if not text.strip():
            logger.warning("‚ö†Ô∏è –¢–µ–∫—Å—Ç –Ω–æ–≤–æ—Å—Ç–∏ –ø—É—Å—Ç–æ–π, —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è –ø—Ä–æ–ø—É—â–µ–Ω–∞")
            ch.basic_ack(delivery_tag=method.delivery_tag)
            return

        # –û—á–µ—Ä–µ–¥—å –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –∏–∑ –ø–æ—Ç–æ–∫–∞
        result_queue = Queue()

        def process_in_thread():
            try:
                # --- Summarization ---
                summary, sum_duration = self.summarizer_manager.summarize(text)
                logger.info(f"[SUMMARY]: {summary}")
                logger.info(f"‚è± –í—Ä–µ–º—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏: {sum_duration:.2f} —Å–µ–∫")

                # --- Categorization ---
                categories, cat_duration = self.categorizer_manager.categorize(summary)
                best_cat = categories[0]["label"] if categories else "–¥—Ä—É–≥–æ–µ"
                score = categories[0]["score"] if categories else 0.0
                logger.info(f"[CATEGORY]: {best_cat} (score={score:.2f})")
                logger.info(f"‚è± –í—Ä–µ–º—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏–∏: {cat_duration:.2f} —Å–µ–∫")

                # --- –§–æ—Ä–º–∏—Ä—É–µ–º JSON ---
                processed_news = {
                    "id": str(uuid.uuid4()),
                    "title": news_data.get('header', ''),
                    "summary": summary,
                    "category": best_cat,
                    "source": news_data.get('source_name', ''),
                    "url": news_data.get('url', ''),
                    "date": news_data.get('date', '')
                }

                result_queue.put(("success", processed_news))
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ç–µ–∫—Å—Ç–∞: {e}", exc_info=True)
                result_queue.put(("error", None))

        # –ó–∞–ø—É—Å–∫ –ø–æ—Ç–æ–∫–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        thread = threading.Thread(target=process_in_thread)
        thread.start()

        # –ü–æ–¥–¥–µ—Ä–∂–∫–∞ heartbeat –≤–æ –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        while thread.is_alive():
            try:
                self.rabbit_connection.process_data_events(time_limit=1)
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤ process_data_events: {e}")
            time.sleep(0.1)

        thread.join()

        # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        status, processed_news = result_queue.get()

        if status == "error":
            ch.basic_nack(delivery_tag=method.delivery_tag, requeue=False)
            return

        # –û—Ç–ø—Ä–∞–≤–∫–∞ –≤ processed_queue
        self.send_to_processed_queue(processed_news, ch)

        # Ack
        ch.basic_ack(delivery_tag=method.delivery_tag)

    def send_to_processed_queue(self, news_json: dict, ch):
        """–û—Ç–ø—Ä–∞–≤–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–π –Ω–æ–≤–æ—Å—Ç–∏ —Å –ø–µ—Ä–µ–ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ–º"""
        while True:
            try:
                ch.basic_publish(
                    exchange='',
                    routing_key=self.processed_queue_name,
                    body=json.dumps(news_json),
                    properties=pika.BasicProperties(delivery_mode=2)
                )
                logger.info(f"üì§ –ù–æ–≤–æ—Å—Ç—å –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–∞ –≤ –æ—á–µ—Ä–µ–¥—å '{self.processed_queue_name}'")
                break
            except (pika.exceptions.ChannelWrongStateError,
                    pika.exceptions.StreamLostError,
                    pika.exceptions.ConnectionClosed) as e:
                logger.warning(f"‚ö†Ô∏è –°–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –ø–æ—Ç–µ—Ä—è–Ω–æ –ø—Ä–∏ –æ—Ç–ø—Ä–∞–≤–∫–µ: {e}, –ø–µ—Ä–µ–ø–æ–¥–∫–ª—é—á–∞–µ–º—Å—è...")
                self.connect_rabbitmq()
                ch = self.channel  # –û–±–Ω–æ–≤–ª—è–µ–º –∫–∞–Ω–∞–ª –ø–æ—Å–ª–µ –ø–µ—Ä–µ–ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è


if __name__ == "__main__":
    with open("config.json", 'r', encoding='utf-8') as file:
        conf = json.load(file)

    consumer = NewsConsumer(conf)
    consumer.start_consuming()