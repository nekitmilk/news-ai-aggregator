import logging
from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM
import time

logger = logging.getLogger("SummarizerManager")
logger.setLevel(logging.INFO)
ch = logging.StreamHandler()
formatter = logging.Formatter("[%(asctime)s] [%(levelname)s] %(message)s")
ch.setFormatter(formatter)
logger.addHandler(ch)

class SummarizerManager:
    def __init__(self, model_name="IlyaGusev/rut5_base_sum_gazeta", device=0):
        logger.info(f"üîÑ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è SummarizerManager...")
        start = time.time()

        logger.info(f"üì• –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä {model_name}...")
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)

        logger.info(f"üì• –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å {model_name}...")
        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

        logger.info(f"‚ö° –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–∞–π–ø–ª–∞–π–Ω summarization –Ω–∞ device={device}...")
        self.summarizer = pipeline(
            "summarization",
            model=self.model,
            tokenizer=self.tokenizer,
            device=device
        )

        logger.info(f"‚úÖ SummarizerManager –≥–æ—Ç–æ–≤ (–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–Ω—è–ª–∞ {time.time() - start:.2f} —Å–µ–∫)")

    def summarize(self, text: str, min_length=30, max_length=100):
        if not text:
            logger.warning("‚ö†Ô∏è –ü–æ–ø—ã—Ç–∫–∞ —Å—É–º–º–∞—Ä–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç")
            return None, 0

        logger.info("üöÄ –ó–∞–ø—É—Å–∫ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏...")
        start_time = time.time()

        summary = self.summarizer(
            text,
            max_length=max_length,
            min_length=min_length,
            do_sample=False
        )

        end_time = time.time()
        logger.info(f"‚úÖ –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {end_time - start_time:.2f} —Å–µ–∫—É–Ω–¥")
        return summary[0]['summary_text'], end_time - start_time
